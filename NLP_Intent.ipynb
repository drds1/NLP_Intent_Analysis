{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Analysis on chat bot data\n",
    "\n",
    "Chat bots are increasingly used to automate online customer queries and negate the need for call center staff.\n",
    "\n",
    "Here we will train a Long Short Term Memory (LSTM) network to suggest the intent of a customer based on the text of the input query.\n",
    "\n",
    "[LSTM](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470) networks are great for text classification problems because they have memory (e.g. would be able to identify the word \"not\" before \"good\" as a negative sentiment). This sequential context is something that humans take for granted but that is very difficult for computers to grasp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test sample\n",
    "\n",
    "We will load separately the train and test data. The Xtrain and ytrain respectively will contain a list of the query (one list element per sample) and the outcome variable (10 possibilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intent analysis atis dataset \n",
    "dftrain = pd.read_csv('./data/datasets_117486_281522_atis.train.csv')\n",
    "Xtrain, ytrain = list(dftrain.values[:,1]), list(dftrain.values[:,-1])\n",
    "\n",
    "labels = list(ytrain.columns)\n",
    "nlabels = len(labels)\n",
    "\n",
    "dftest = pd.read_csv('./data/datasets_117486_281522_atis.test.csv')\n",
    "Xtest, ytest = list(dftest.values[:,1]), list(dftest.values[:,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode labels\n",
    "\n",
    "We will be predicting the probability that an input beloings to each class. For each label, we therefore need a vector of 1s and 0s where 1 appears next to the correct label, and all incorrect labels are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode labnels\n",
    "ytrain = pd.get_dummies(ytrain)\n",
    "ytrain = np.array(ytrain)\n",
    "ytest = pd.get_dummies(ytest)\n",
    "head(ytest)\n",
    "ytest = np.array(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "We now map each word to an integer identifier. The list sample is then converted into a vector of integers corresponding to each word. For shorter inputs, we padd the start with zeros so that each sample has the same input vector length. \n",
    "\n",
    "Since there are so many words collectively in the full dataset, we keep only the 50 most common in each sample to avoid overfitting our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all abstracts to sequences of integers key stored in idx_word\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=50,\n",
    "                                               filters='?!\":;,.#$&()*+-<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                                               lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "#assign an integer ID to each word\n",
    "Xtrain_sequence = tokenizer.texts_to_sequences(Xtrain)\n",
    "#padd the sequences of short sentences with 0s so everything is the same length\n",
    "Xtrain_sequence = keras.preprocessing.sequence.pad_sequences(Xtrain_sequence)\n",
    "\n",
    "#record the word to ID map and count the number of words in our vocabulary (+ 1 as we have the 0 padding as a word)\n",
    "idx_word = tokenizer.index_word\n",
    "num_words = len(idx_word) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "We need a mathematical way to represent 'words' in vector form such that words with similar meaning have vectors that point in similar directions. i.e. \"plant\" and \"flower\" would have similar pointing vectors in this abstract \"Embedding Vector Space\", but that vectors for words like \"Hot\" and \"Cold\" would point in opposite directions. Training these word embeddings is a herculean task for GPU's. Fortunately, other boffins have done the job for us and we can load a pre-trained word embedding dictionary. The 'glove' 100d word embeddings represents words in a 100 dimensional vector space. The more dimensions, the better linguistic understanding of our classifier, but the more compute time and sample size is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in word embeddings\n",
    "embeddings_dict = load_embeddings('../disaster_nlp/data/non_tracked/glove.6B.100d.txt')\n",
    "embeddings_words = list(embeddings_dict.keys())\n",
    "wordvec_dim = embeddings_dict[embeddings_words[0]].shape[0]\n",
    "embedding_matrix = np.zeros((num_words,wordvec_dim))\n",
    "for i, word in enumerate(idx_word.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = embeddings_dict.get(word, None)\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model\n",
    "\n",
    "We now build the LSTM model. Many neural nets share similar features. If we want to assign the probability to each class of a given input, the output will always be a Dense layer with softmax activation function equal to the number of labels. If we want a positive / negative decision, the output will be a single neuron with a sigmoid loss. The differences with LSTMs are the input 'Embedding' layer, where we specify our newly loaded word embeddings, and the number of input training samples and size of our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise the usual sequential network\n",
    "model_lstm = keras.Sequential()\n",
    "\n",
    "#initialise Ebedding layer\n",
    "# input_length is the number of words ids per sample e.g 28\n",
    "# NOT the sample size of the training data\n",
    "# you do not need to supply that info\n",
    "model_lstm.add(keras.layers.Embedding(input_dim=num_words,\n",
    "                                      input_length=Xtrain_sequence.shape[1],\n",
    "                                      output_dim=wordvec_dim,\n",
    "                                      weights=[embedding_matrix],\n",
    "                                      trainable=False,\n",
    "                                      mask_zero=True))\n",
    "\n",
    "#words which are not in the pretrained embeddings (with value 0) are ignored\n",
    "model_lstm.add(keras.layers.Masking(mask_value = 0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model_lstm.add(keras.layers.LSTM(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularisation and avoid overfit\n",
    "model_lstm.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model_lstm.add(keras.layers.Dense(nlabels,activation = 'softmax' ))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summarise the model\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.fit(Xtrain_sequence, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the test data using the tokenizer and evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the test data\n",
    "Xtest_sequence = tokenizer.texts_to_sequences(Xtest)\n",
    "Xtest_sequence = keras.preprocessing.sequence.pad_sequences(Xtest_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model performance\n",
    "eval_lstm = model_lstm.evaluate(Xtest_sequence, ytest)\n",
    "ypred = model_lstm.predict(Xtest_sequence, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
